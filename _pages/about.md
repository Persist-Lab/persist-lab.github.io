---
permalink: /
title: "Pervasive Intelligent Systems (Persist) Lab"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

 
<p align="justify">

  
AI holds great promise for improving human communication and interaction. However, data-driven solutions often overlook key human-centric challenges, such as ambiguous and fragmented information and implicit contexts. At the Pervasive Intelligent Systems Lab (PersistLab), we develop <b>socio-technical AI methods and systems</b> that enhance communication, decision-making, and well-being in high-stakes domains such as healthcare. Our research bridges <b>natural language processing, human–AI interaction, and computational health</b> to <b>develop</b> and <b>validate</b> novel methods, and systems that understand context, uncertainty, and human intent. We study how people communicate about health by collecting and analyzing multimodal data from patient–provider portals, online health communities, and social and behavioral sensing. We create domain informed representation of data by engaging with different stakeholders, including patients, clinicians, patient advocates, community engagement partners, and clinical and public health researchers. Supported by NIH, NSF, Google, CTSA, and institutional fundings, our work advances the science of <b>collaborative intelligence</b>, where humans and AI systems work together to improve <b>health communication, coordination,</b> and <b>outcomes</b>.

</p> 

<figure>
  
  <img src="/images/overviewResearch.png" alt="Overview of research" style="width:100%">

  <figcaption>Overview of my current and prior work, and how it informs my long-term vision.</figcaption>
  
</figure>

## <font> News and Announcements </font>
<div style="height: 450px; overflow: auto;">

<font color="brown"><i>15 September, 2025</i></font> <br/>
<font> Congratulations to Omar for the EMNLP 2025 findngs paper: <a href="https://arxiv.org/pdf/2502.16838" style="text-decoration: none"> REGen: A Reliable Evaluation Framework for Generative Event Argument Extraction</a>.
</font> <br/>

  
<font color="brown"><i>10 June, 2025</i></font> <br/>
<font> Excited to receive the <a href="https://research.google/programs-and-events/research-scholar-program/recipients/">Google Research Scholar Award</a> to investigate how to safely situate LLMs for improved patient-provider communication.
</font> <br/>

<font color="brown"><i>15 May, 2025</i></font> <br/>
<font> Congratulations to Joey et al. for the two <a href="https://2025.aclweb.org/" style="text-decoration: none"> ACL 2025</a> papers in the main track: <a href="https://aclanthology.org/2025.acl-long.1226.pdf" style="text-decoration: none"> Follow-up Question Generation For Enhanced Patient-Provider Conversations</a> and <a href="https://aclanthology.org/2025.acl-long.1221.pdf" style="text-decoration: none"> Document-Level Event-Argument Data Augmentation for Challenging Role Types</a>.
</font> <br/>
  
  <font color="brown"><i>23 September, 2024</i></font> <br/>
<font> Our NIH R21 has been selected for funding (Total funding: $451K, Role: PI, Title: "Characterizing Information Needs and Peer Engagement Regarding
Medication for Opioid Use Disorder on Social Media"). I am thankful to my co-Is Dr. Sarah Lord, Dr. Jacob Borodobsky, and mentor Dr. Lisa Marsch, for their support. We will build human-centric NLP pipelines to analyze online discourse on recovery treatment and improve patient education and communication. Thank you, NIH!
</font> <br/>

<font color="brown"><i>20 September, 2024</i></font> <br/>
<font> Congratulations to Omar et al. for the <a href="https://2024.emnlp.org/" style="text-decoration: none"> EMNLP 2024</a> paper: <a href="https://omar-sharif03.github.io/DiscourseEE/" style="text-decoration: none"> Explicit, Implicit, and Scattered: Revisiting Event Extraction to Capture Complex Arguments</a>.
</font> <br/>

<font color="brown"><i>30 July, 2024</i></font> <br/>
<font> Congratulations to Madhusudan and others.  Our group is in <a href="https://openai.com/form/researcher-access-program/">Researcher Access Program</a>. We received a $5k API credit and will continue to assess OpenAI models on human-centric NLP tasks. 
</font> <br/>

<font color="brown"><i>25 July, 2024</i></font> <br/>
<font> Dartmouth SYNERGY team received <a href="https://geiselmed.dartmouth.edu/news/2024/28m-federal-grant-to-fund-medical-innovations-from-dartmouth-health-research/">a $27.7M funding through NIH’s Clinical and Translational Science Award</a> (PIs: Drs. Steven L. Bernstein, Anna N.A. Tosteson, Keith D. Paulsen). As a co-PI, my students and I will work on the development of an NLP pipeline to analyze pre-visit questionnaire responses and clinical notes, aiming to assess goal-aligned care.
</font> <br/>
  
<font color="brown"><i>10 June, 2024</i></font> <br/>
<font> Excited to receive the <a href="https://www.dartmouth-hitchcock.org/hitchcock-foundation/pilot-research-grants">Hitchcock Foundation Pilot Research Grant</a>
 ($50K, role: PI). In collaboration with DHMC providers, we will develop human-centric NLP solutions to triage patient portal messages. We also made it to the final round of the <a href="https://www.c4tbh.org/accelerator/">DIADH Accelerator</a>.
</font> <br/>

<font color="brown"><i>4 June, 2024</i></font> <br/>
<font> We successfully wrapped the <a href="https://sites.google.com/view/real-info-2024/overview" style="text-decoration: none">Reliable Evaluation of LLMs for Factual Information (REAL-Info)</a> workshop at <a href="https://www.icwsm.org/2024/index.html/" style="text-decoration: none">ICWSM-2024</a>. Thanks to my wonderful co-organizers, students, and all our participants. Special thanks to Dr. Munmun De Choudhury for her amazing keynote!
</font> <br/>

<font color="brown"><i>16 May, 2024</i></font> <br/>
<font> Congratulations to Omar et al. for the <a href="https://2024.aclweb.org/" style="text-decoration: none"> ACL 2024</a> paper: <a href="https://aclanthology.org/2024.acl-long.454/" style="text-decoration: none"> Deciphering Hate: Identifying Hateful Memes and Their Targets</a>.
</font> <br/>

<font color="brown"><i>21 April, 2024</i></font> <br/>
<font> Check out our paper  <a href="https://workshop-proceedings.icwsm.org/pdf/2024_33.pdf" style="text-decoration: none">Do LLMs Find Human Answers To Fact-Driven Questions Perplexing? A Case Study on Reddit</a>, at <a href="https://sites.google.com/view/real-info-2024/overview" style="text-decoration: none"> Reliable Evaluation of LLMs for Factual Information (REAL-Info)</a> workshop, co-located with <a href="https://www.icwsm.org/2024/index.html/" style="text-decoration: none">ICWSM-2024</a>.
</font> <br/>

<font color="brown"><i>8 March, 2024</i></font> <br/>
<font> We have posted two new pre-prints on arXiv! (1) <a href="https://arxiv.org/abs/2403.03304" style="text-decoration: none">Mad Libs Are All You Need: Augmenting Cross-Domain Document-Level Event Argument Data</a> (2) <a href="https://arxiv.org/abs/2403.03336" style="text-decoration: none">Scope of Large Language Models for Mining Emerging Opinions in Online Health Discourse</a>. 
</font> <br/>

<!--
<font color="brown"><i>10 June, 2024</i></font> <br/>
<font> Our project on triaging patient portal messages made it to the final round of the <a href="https://www.c4tbh.org/accelerator/">DIADH Accelerator</a>
</font> <br/>

<font color="brown"><i>31 January, 2024</i></font> <br/>
<font> Our workshop, titled <a href="https://sites.google.com/view/real-info-2024/overview" style="text-decoration: none">Reliable Evaluation of LLMs for Factual Information (REAL-Info)</a>, has been accepted at <a href="https://www.icwsm.org/2024/index.html/" style="text-decoration: none">ICWSM-2024</a>.  Please consider submitting your relevant work to this workshop!
</font> <br/>
-->



<font color="brown"><i>17 January, 2024</i></font> <br/>
<font> One paper on Multimodal Learning accepted at <a href="https://sites.google.com/view/eacl2024srw" style="text-decoration: none">EACL-2024 SRW</a>. Congratulations to Eftekhar, Omar, and other co-authors. 
</font> <br/>

<font color="brown"><i>09 December, 2023</i></font> <br/>
<font> One paper accepted at <a href="https://aaai.org/aaai-conference/" style="text-decoration: none">AAAI-2024</a>. Congratulations to Omar, Madhu, and other co-authors. 
</font> <br/>
  
<font color="brown"><i>23 October, 2023</i></font> <br/>
<font> One paper accepted in <a href="https://gem-benchmark.com/workshop" style="text-decoration: none">GEM Workshop</a> at <a href="https://2023.emnlp.org/" style="text-decoration: none">EMNLP-2023</a>. Congratulations to Joey and other co-authors. 
</font> <br/>
  
<font color="brown"><i>06 October, 2023</i></font> <br/>
<font> Two papers accepted at <a href="https://2023.emnlp.org/" style="text-decoration: none">EMNLP-2023</a>. Congratulations to Joey, Parker, and Omar!
</font> <br/>

<font color="brown"><i>15 July, 2023</i></font> <br/>
<font> One <a href="https://arxiv.org/abs/2301.11508" style="text-decoration: none"><font>paper</font></a> accepted at <a href="https://www.icwsm.org/2023/index.html/call_for_submissions.html" style="text-decoration: none">ICWSM-2024</a>. Congratulations to Will, Omar, Madhu, and other authors!
</font> <br/>

<font color="brown"><i>26 June, 2023</i></font> <br/>
<font> Congratulations to Parker for presenting his <a href="https://arxiv.org/pdf/2303.09366.pdf" style="text-decoration: none"><font>paper</font></a> in <a href="https://ieeeichi.github.io/ICHI2023/" style="text-decoration: none"><font>ICHI-23</font></a>.
</font> <br/>

<font color="brown"><i>5 June, 2023</i></font> <br/>
<font> Congratulations to Joey and Parker for presenting their papers (<a href="https://ojs.aaai.org/index.php/ICWSM/article/view/22140" style="text-decoration: none"><font>Paper-1</font></a>, <a href="https://ojs.aaai.org/index.php/ICWSM/article/view/22210" style="text-decoration: none"><font>Paper-2</font></a>) in <a href="https://www.icwsm.org/2023/index.html/index.html" style="text-decoration: none"><font>ICWSM-23</font></a>.
</font> <br/>

<font color="brown"><i>12 September, 2022</i></font> <br/>
<font> Congratulations to Omar for being awarded the <b>Presidential Graduate Fellowship</b> from Dartmouth. 
</font> <br/>


<font color="brown"><i>10 September, 2022</i></font> <br/>
<font> Welcome to our new PhD students, Madhusudan Basak and Omar Sharif!
</font> <br/>

<font color="brown"><i>21 August, 2022</i></font> <br/>
<font> Welcome to Lutz Lu, Vasavi Garimella, Dae Lim Chung, Vasavi Garimella, Garrett Johnston, Burke Jaeger, Love Tsai, Zhanel Nugmanova, who have joined the PersistLab!
</font> <br/>

<font color="brown"><i>22 May, 2022</i></font> <br/>
<font> Congratulations to Parker on being named a <a href="https://graduate.dartmouth.edu/academics/programs/phd-innovation-program-dartmouth" style="text-decoration: none"><font>Guarini PhD Innovation Fellow</font></a>.
</font>
</div>


